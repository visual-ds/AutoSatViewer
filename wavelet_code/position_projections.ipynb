{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3706/406383291.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from matplotlib.colors import rgb2hex\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "\n",
    "from ae import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colors_polygons(poly_division):\n",
    "    file = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\").to_crs(\"EPSG:6933\")\n",
    "    centroids = file.centroid.apply(lambda x: (x.x, x.y)).to_list()\n",
    "    centroids = np.array(centroids)\n",
    "    for i in range(2):\n",
    "        centroids[:, i] -= centroids[:, i].min()\n",
    "        centroids[:, i] /= centroids[:, i].max()\n",
    "        centroids[:, i] *= 511\n",
    "    centroids = centroids.astype(int)\n",
    "\n",
    "    img = Image.open(\"data/misc/bremm.png\")\n",
    "    img = np.array(img)\n",
    "\n",
    "    colors = []\n",
    "    for i in centroids:\n",
    "        colors.append(rgb2hex(img[i[1], i[0]] / 255))\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args('')\n",
    "args.batch_size = 32\n",
    "args.embedding_dim = 64\n",
    "args.n_layers = 1\n",
    "args.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "args.dropout = False\n",
    "args.use_bn = False\n",
    "args.lr = 0.001\n",
    "args.epoch = 200\n",
    "\n",
    "\n",
    "def projection_coeffs_deep(poly_division, time_interval, spatial = False):\n",
    "    if spatial:\n",
    "        coeffs = pd.read_csv(f\"data/coeffs_spatial/{poly_division}_{time_interval}.csv\")\n",
    "    else:\n",
    "        coeffs = pd.read_csv(f\"data/coeffs/{poly_division}_{time_interval}.csv\")\n",
    "\n",
    "    colors = get_colors_polygons(poly_division)\n",
    "    ts = []\n",
    "    for v in coeffs.type.unique():\n",
    "        ts.append(coeffs[coeffs.type == v].pivot(index = \"id_poly\", columns = \"date\", values = [\"mean_freq_3\"]).values)\n",
    "    ts = np.array(ts)\n",
    "    ts = ts.transpose(1, 0, 2)\n",
    "    ts = [t for t in ts]\n",
    "\n",
    "\n",
    "    train_idx = np.random.choice(len(ts), int(0.8 * len(ts)), replace = False)\n",
    "    val_idx = np.array([i for i in range(len(ts)) if i not in train_idx])\n",
    "    ts_train = [torch.tensor(ts[i], dtype = torch.float32) for i in train_idx]\n",
    "    ts_val = [torch.tensor(ts[i], dtype = torch.float32) for i in val_idx]\n",
    "    ts_train = DataLoader(ts_train, batch_size = args.batch_size, shuffle = False)\n",
    "    ts_val = DataLoader(ts_val, batch_size = args.batch_size, shuffle = False)\n",
    "\n",
    "\n",
    "    model = AutoencoderConv(\n",
    "        input_dim = ts[0].shape[0],\n",
    "        encoding_dim = args.embedding_dim,\n",
    "        seq_len = ts[0].shape[1],\n",
    "        h_dims = [128, 64],\n",
    "        h_activ = nn.ReLU(),\n",
    "        out_activ = nn.Identity()\n",
    "    )\n",
    "\n",
    "    train_loss, val_loss = train_model(\n",
    "        model,\n",
    "        ts_train,\n",
    "        ts_val,\n",
    "        args,\n",
    "        verbose = False\n",
    "    )\n",
    "\n",
    "    ts_train = [torch.tensor(t, dtype = torch.float32) for t in ts]\n",
    "    ts_train = DataLoader(ts_train, batch_size = args.batch_size, shuffle = False)\n",
    "\n",
    "    encodings = get_encodings(model, ts_train, args)\n",
    "        \n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    proj = tsne.fit_transform(encodings)\n",
    "\n",
    "    for i in range(2):\n",
    "        proj[:, i] -= proj[:, i].min()\n",
    "        #proj[:, i] += 0.05\n",
    "        proj[:, i] /= proj[:, i].max()\n",
    "\n",
    "    projections = pd.DataFrame(proj, columns = [\"x\", \"y\"])\n",
    "    projections[\"id_poly\"] = np.arange(len(ts))\n",
    "    projections[\"color\"] = colors\n",
    "\n",
    "    ts = np.array(ts) # (n, n_f, t)\n",
    "    # for each n and n_f, compute the mean of the time series\n",
    "    mean_ts = ts.mean(axis = 2)\n",
    "    for i, v in enumerate(coeffs.type.unique()):\n",
    "        projections[v] = mean_ts[:, i]\n",
    "\n",
    "    if spatial:\n",
    "        projections.to_csv(f\"data/projections_spatial/{poly_division}_{time_interval}.csv\", index=False)\n",
    "    else:\n",
    "        projections.to_csv(f\"data/projections/{poly_division}_{time_interval}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_coeffs_deep(\"SpCenterCensus5k\", \"Period1\")\n",
    "projection_coeffs_deep(\"SpCenterCensus5k\", \"Period1\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_coeffs_deep(\"SpCenterCensus5k\", \"Period2\")\n",
    "projection_coeffs_deep(\"SpCenterCensus5k\", \"Period2\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_coeffs_deep(\"NYBlocks\", \"Period1\")\n",
    "projection_coeffs_deep(\"NYBlocks\", \"Period1\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_coeffs_deep(\"BLACities\", \"Year\")\n",
    "projection_coeffs_deep(\"BLACities\", \"Year\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_coeffs_deep(\"BLACities\", \"Year2\")\n",
    "projection_coeffs_deep(\"BLACities\", \"Year2\", True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavelet_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
