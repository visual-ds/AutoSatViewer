{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133415/1767567661.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from shapely.geometry import Point\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossref(df, gdf):\n",
    "    \"\"\"\n",
    "    Link rows of a dataframe to polygons in a geodataframe.\n",
    "    Dataframe must have columns 'LONGITUDE' and 'LATITUDE'.\n",
    "    \"\"\"\n",
    "    shapes = gdf.geometry\n",
    "    centroids = [x.centroid for x in shapes]\n",
    "    centroids = np.array([(x.x, x.y) for x in centroids])\n",
    "    nn = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(centroids)\n",
    "    _, indices = nn.kneighbors(df[['LONGITUDE', 'LATITUDE']])\n",
    "\n",
    "    id_poly = []\n",
    "    i = 0\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        point = Point(row['LONGITUDE'], row['LATITUDE'])\n",
    "        new_id = None\n",
    "        for idx in indices[i]:\n",
    "            if shapes[idx].contains(point):\n",
    "                new_id = idx\n",
    "                break\n",
    "        id_poly.append(new_id)\n",
    "        i += 1\n",
    "\n",
    "    df['id_poly'] = id_poly\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## São Paulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_range = [\"2019-12-01\", \"2020-02-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_mapper = {\n",
    "    \"A NOITE\" : \"21:00:00\",\n",
    "    \"A TARDE\" : \"15:00:00\",\n",
    "    \"DE MADRUGADA\" : \"03:00:00\",\n",
    "    \"PELA MANHÃ\" : \"09:00:00\",\n",
    "    \"EM HORA INCERTA\" : None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_dates(day_range):\n",
    "    dates = pd.date_range(day_range[0], day_range[1], freq=\"D\")\n",
    "    # repeat with hours: 3h, 9h, 15h, 18h\n",
    "    new_dates = []\n",
    "    for d in dates:\n",
    "        new_dates.append(d + pd.Timedelta(hours=3))\n",
    "        new_dates.append(d + pd.Timedelta(hours=9))\n",
    "        new_dates.append(d + pd.Timedelta(hours=15))\n",
    "        new_dates.append(d + pd.Timedelta(hours=21))\n",
    "    return new_dates\n",
    "\n",
    "def get_all_dates_2(day_range):\n",
    "    dates = pd.date_range(day_range[0], day_range[1], freq=\"D\")\n",
    "    # repeat with hours: 3h, 9h, 15h, 18h\n",
    "    new_dates = []\n",
    "    for d in dates:\n",
    "        new_dates.append(d + pd.Timedelta(hours=0))\n",
    "        new_dates.append(d + pd.Timedelta(hours=12))\n",
    "    return new_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_date(x):\n",
    "    hour = x.hour\n",
    "    if 6 <= hour < 12:\n",
    "        new_hour = 9\n",
    "    elif 12 <= hour < 18:\n",
    "        new_hour = 15\n",
    "    elif 18 <= hour < 24:\n",
    "        new_hour = 21\n",
    "    else:\n",
    "        new_hour = 3\n",
    "    return pd.to_datetime(f\"{x.date()} {new_hour}:00:00\")\n",
    "\n",
    "def simplify_date_2(x):\n",
    "    hour = x.hour\n",
    "    if 6 <= hour < 18:\n",
    "        new_hour = 12\n",
    "    else:\n",
    "        new_hour = 0\n",
    "    return pd.to_datetime(f\"{x.date()} {new_hour}:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_division = \"SpCenterCensus2k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000000\n",
    "df = pd.read_csv(\"data/time_series/waze-alerts.csv\")\n",
    "df = df[[\"geo\", \"ts\", \" type\"]]\n",
    "df = df.rename(columns = {\" type\" : \"type\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"ts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.date >= day_range[0]]\n",
    "df = df[df.date < day_range[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"LONGITUDE\"] = df[\"geo\"].apply(lambda x : x.split(\"(\")[1].split(\" \")[0])\n",
    "df[\"LATITUDE\"] = df[\"geo\"].apply(lambda x : x.split(\" \")[1].split(\")\")[0])\n",
    "df.LONGITUDE = df.LONGITUDE.astype(float)\n",
    "df.LATITUDE = df.LATITUDE.astype(float)\n",
    "df[\"type\"] = df[\"type\"].apply(lambda x : \"ROADCLOSED\" if x == \"ROAD_CLOSED\" else x)\n",
    "df = df[df.date >= day_range[0]]\n",
    "df = df[df.date < day_range[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovani/anaconda3/envs/wavelet_code/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but NearestNeighbors was fitted without feature names\n",
      "  warnings.warn(\n",
      "100%|██████████| 1686772/1686772 [02:48<00:00, 9995.70it/s] \n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "df = crossref(df, gdf)\n",
    "df = df.dropna(subset = [\"id_poly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"updated_date\"] = df[\"date\"].apply(simplify_date)\n",
    "df[\"id_poly\"] = df[\"id_poly\"].astype(int)\n",
    "n_poly = len(gdf)\n",
    "all_dates = get_all_dates(day_range)\n",
    "n_days = len(all_dates)\n",
    "\n",
    "for t in df.type.unique():\n",
    "    df_ = df[df.type == t].copy()\n",
    "    df_ = df_.groupby([\"id_poly\", \"updated_date\"]).size().reset_index()\n",
    "    ts = np.zeros((n_poly, n_days))\n",
    "    for i, d in enumerate(all_dates):\n",
    "        filtered = df_[df_.updated_date == d]\n",
    "        if len(filtered) == ts.shape[0]:\n",
    "            ts[:, i] = filtered[0]\n",
    "        else:\n",
    "            ts[filtered.id_poly, i] = filtered[0]\n",
    "    \n",
    "    area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "    ts = ts / area[:, None]\n",
    "    np.save(f\"data/time_series/{t}_{poly_division}_Period1.npy\", ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"updated_date\"] = df[\"date\"].apply(simplify_date_2)\n",
    "df[\"id_poly\"] = df[\"id_poly\"].astype(int)\n",
    "n_poly = len(gdf)\n",
    "all_dates = get_all_dates_2(day_range)\n",
    "n_days = len(all_dates)\n",
    "\n",
    "for t in df.type.unique():\n",
    "    df_ = df[df.type == t].copy()\n",
    "    df_ = df_.groupby([\"id_poly\", \"updated_date\"]).size().reset_index()\n",
    "    ts = np.zeros((n_poly, n_days))\n",
    "    for i, d in enumerate(all_dates):\n",
    "        filtered = df_[df_.updated_date == d]\n",
    "        if len(filtered) == ts.shape[0]:\n",
    "            ts[:, i] = filtered[0]\n",
    "        else:\n",
    "            ts[filtered.id_poly, i] = filtered[0]\n",
    "    \n",
    "    area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "    ts = ts / area[:, None]\n",
    "    np.save(f\"data/time_series/{t}_{poly_division}_Period2.npy\", ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime theft and robbery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_theft():\n",
    "    df = pd.read_csv(\"data/time_series/furto_celular_2018_2022.csv\", sep = \";\")\n",
    "    df[\"hour\"] = df.PERIDOOCORRENCIA.apply(lambda x : hour_mapper[x])\n",
    "    df[\"date\"] = pd.to_datetime(df.DATAOCORRENCIA, format='%d/%m/%Y', errors='coerce')\n",
    "    df[\"date\"] = df[\"date\"] + pd.to_timedelta(df[\"hour\"], errors='coerce')\n",
    "    print(f\"Fraction of nan dates: {df.date.isna().mean():.2f}\")\n",
    "    df = df.dropna(subset = [\"date\"])\n",
    "    df = df[[\"date\", \"LATITUDE\", \"LONGITUDE\"]]\n",
    "    gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "    df = crossref(df, gdf)\n",
    "    df = df.dropna(subset = [\"id_poly\"])\n",
    "    df[\"id_poly\"] = df[\"id_poly\"].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_707069/2194761068.py:2: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/time_series/furto_celular_2018_2022.csv\", sep = \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of nan dates: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovani/anaconda3/envs/wavelet_code/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but NearestNeighbors was fitted without feature names\n",
      "  warnings.warn(\n",
      "100%|██████████| 239651/239651 [00:25<00:00, 9239.66it/s]\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "df = load_theft()\n",
    "df = df[df.date >= day_range[0]]\n",
    "df = df[df.date < day_range[1]]\n",
    "n_poly = len(gdf)\n",
    "all_dates = get_all_dates(day_range)\n",
    "n_days = len(all_dates)\n",
    "df = df.groupby([\"id_poly\", \"date\"]).size().reset_index()\n",
    "\n",
    "ts = np.zeros((n_poly, n_days))\n",
    "for i, d in enumerate(all_dates):\n",
    "    filtered = df[df.date == d]\n",
    "    if len(filtered) == ts.shape[0]:\n",
    "        ts[:, i] = filtered[0]\n",
    "    else:\n",
    "        ts[filtered.id_poly, i] = filtered[0]\n",
    "\n",
    "area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "ts = ts / area[:, None]\n",
    "np.save(f\"data/time_series/FurtoCelular_{poly_division}_Period1.npy\", ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_707069/2194761068.py:2: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/time_series/furto_celular_2018_2022.csv\", sep = \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of nan dates: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovani/anaconda3/envs/wavelet_code/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but NearestNeighbors was fitted without feature names\n",
      "  warnings.warn(\n",
      "100%|██████████| 239651/239651 [00:26<00:00, 9151.02it/s]\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "df = load_theft()\n",
    "df = df[df.date >= day_range[0]]\n",
    "df = df[df.date < day_range[1]]\n",
    "df[\"date\"] = df[\"date\"].apply(simplify_date_2)\n",
    "\n",
    "n_poly = len(gdf)\n",
    "all_dates = get_all_dates_2(day_range)\n",
    "n_days = len(all_dates)\n",
    "df = df.groupby([\"id_poly\", \"date\"]).size().reset_index()\n",
    "\n",
    "ts = np.zeros((n_poly, n_days))\n",
    "for i, d in enumerate(all_dates):\n",
    "    filtered = df[df.date == d]\n",
    "    if len(filtered) == ts.shape[0]:\n",
    "        ts[:, i] = filtered[0]\n",
    "    else:\n",
    "        ts[filtered.id_poly, i] = filtered[0]\n",
    "\n",
    "area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "ts = ts / area[:, None]\n",
    "np.save(f\"data/time_series/FurtoCelular_{poly_division}_Period2.npy\", ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime robbery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_robbery():\n",
    "    df = pd.read_csv(\"data/time_series/roubo_celular_2018_2022.csv\", sep = \";\")\n",
    "    df[\"hour\"] = df.PERIDOOCORRENCIA.apply(lambda x : hour_mapper[x])\n",
    "    df[\"date\"] = pd.to_datetime(df.DATAOCORRENCIA, format='%d/%m/%Y', errors='coerce')\n",
    "    df[\"date\"] = df[\"date\"] + pd.to_timedelta(df[\"hour\"], errors='coerce')\n",
    "    print(f\"Fraction of nan dates: {df.date.isna().mean():.2f}\")\n",
    "    df = df.dropna(subset = [\"date\"])\n",
    "    df = df[[\"date\", \"LATITUDE\", \"LONGITUDE\"]]\n",
    "    gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "    df = crossref(df, gdf)\n",
    "    df = df.dropna(subset = [\"id_poly\"])\n",
    "    df[\"id_poly\"] = df[\"id_poly\"].astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_707069/1937086392.py:2: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/time_series/roubo_celular_2018_2022.csv\", sep = \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of nan dates: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovani/anaconda3/envs/wavelet_code/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but NearestNeighbors was fitted without feature names\n",
      "  warnings.warn(\n",
      "100%|██████████| 623682/623682 [01:04<00:00, 9618.97it/s]\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "df = load_robbery()\n",
    "df = df[df.date >= day_range[0]]\n",
    "df = df[df.date < day_range[1]]\n",
    "n_poly = len(gdf)\n",
    "n_poly = len(gdf)\n",
    "all_dates = get_all_dates(day_range)\n",
    "n_days = len(all_dates)\n",
    "df = df.groupby([\"id_poly\", \"date\"]).size().reset_index()\n",
    "ts = np.zeros((n_poly, n_days))\n",
    "for i, d in enumerate(all_dates):\n",
    "    filtered = df[df.date == d]\n",
    "    if len(filtered) == ts.shape[0]:\n",
    "        ts[:, i] = filtered[0]\n",
    "    else:\n",
    "        ts[filtered.id_poly, i] = filtered[0]\n",
    "\n",
    "area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "ts = ts / area[:, None]\n",
    "np.save(f\"data/time_series/RouboCelular_{poly_division}_Period1.npy\", ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_707069/1937086392.py:2: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/time_series/roubo_celular_2018_2022.csv\", sep = \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of nan dates: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovani/anaconda3/envs/wavelet_code/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but NearestNeighbors was fitted without feature names\n",
      "  warnings.warn(\n",
      "100%|██████████| 623682/623682 [01:05<00:00, 9542.87it/s]\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "df = load_robbery()\n",
    "df = df[df.date >= day_range[0]]\n",
    "df = df[df.date < day_range[1]]\n",
    "df[\"date\"] = df[\"date\"].apply(simplify_date_2)\n",
    "n_poly = len(gdf)\n",
    "all_dates = get_all_dates_2(day_range)\n",
    "n_days = len(all_dates)\n",
    "df = df.groupby([\"id_poly\", \"date\"]).size().reset_index()\n",
    "ts = np.zeros((n_poly, n_days))\n",
    "for i, d in enumerate(all_dates):\n",
    "    filtered = df[df.date == d]\n",
    "    if len(filtered) == ts.shape[0]:\n",
    "        ts[:, i] = filtered[0]\n",
    "    else:\n",
    "        ts[filtered.id_poly, i] = filtered[0]\n",
    "\n",
    "area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "ts = ts / area[:, None]\n",
    "np.save(f\"data/time_series/RouboCelular_{poly_division}_Period2.npy\", ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_naming = {\n",
    "    \"JAM\": \"Jam\",\n",
    "    \"ACCIDENT\": \"Accident\",\n",
    "    \"ROADCLOSED\": \"Road Closed\",\n",
    "    \"WEATHERHAZARD\": \"Weather Hazard\",\n",
    "    \"FurtoCelular\": \"Phone Theft\",\n",
    "    \"RouboCelular\": \"Phone Robbery\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(504000, 8)\n"
     ]
    }
   ],
   "source": [
    "ts = [np.load(f\"data/time_series/{key}_{poly_division}_Period1.npy\") for key in features_naming.keys()]\n",
    "ts_sum = [np.sum(t) for t in ts]\n",
    "ts = [t for i, t in enumerate(ts) if ts_sum[i] > 0]\n",
    "signals = [s for i, s in enumerate(features_naming.keys()) if ts_sum[i] > 0]\n",
    "date = get_all_dates(day_range)\n",
    "df = []\n",
    "for poly in range(ts[0].shape[0]):\n",
    "    for t in range(ts[0].shape[1]):\n",
    "        df.append({\n",
    "            \"date\" : date[t],\n",
    "            \"id_poly\" : poly,\n",
    "        })\n",
    "        for i, signal in enumerate(signals):\n",
    "            df[-1][\n",
    "                features_naming[signal]\n",
    "            ] = ts[i][poly, t]\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "print(df.shape)\n",
    "df.to_csv(f\"data/polygon_data/{poly_division}_Period1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(252000, 8)\n"
     ]
    }
   ],
   "source": [
    "ts = [np.load(f\"data/time_series/{key}_{poly_division}_Period2.npy\") for key in features_naming.keys()]\n",
    "ts_sum = [np.sum(t) for t in ts]\n",
    "ts = [t for i, t in enumerate(ts) if ts_sum[i] > 0]\n",
    "signals = [s for i, s in enumerate(features_naming.keys()) if ts_sum[i] > 0]\n",
    "date = get_all_dates_2(day_range)\n",
    "df = []\n",
    "for poly in range(ts[0].shape[0]):\n",
    "    for t in range(ts[0].shape[1]):\n",
    "        df.append({\n",
    "            \"date\" : date[t],\n",
    "            \"id_poly\" : poly,\n",
    "        })\n",
    "        for i, signal in enumerate(signals):\n",
    "            df[-1][\n",
    "                features_naming[signal]\n",
    "            ] = ts[i][poly, t]\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "print(df.shape)\n",
    "df.to_csv(f\"data/polygon_data/{poly_division}_Period2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = \"april\"\n",
    "#df = pd.read_csv(f\"data/time_series/2016_{month}_Yellow_Taxi_Trip_Data.csv\")\n",
    "df = pd.read_csv(f\"data/time_series/2016_Yellow_Taxi_Trip_Data_end_april.csv\")\n",
    "#df = df[df.trip_distance < 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"amount\"] = df.fare_amount + df.extra + df.mta_tax + df.improvement_surcharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133415/984513517.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"date\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"])\n"
     ]
    }
   ],
   "source": [
    "df[\"LONGITUDE\"] = df[\"pickup_longitude\"]\n",
    "df[\"LATITUDE\"] = df[\"pickup_latitude\"]\n",
    "df[\"date\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"])\n",
    "df = df.drop(columns = [\n",
    "    'VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
    "    'pickup_longitude', 'pickup_latitude', 'RatecodeID', 'store_and_fwd_flag',\n",
    "    'dropoff_longitude', 'dropoff_latitude', 'payment_type',  'extra', \n",
    "    'mta_tax', 'tolls_amount', 'improvement_surcharge', 'fare_amount', 'PULocationID', 'DOLocationID', \"total_amount\"], \n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_range = [\"2016-04-26\", \"2016-05-03\"]\n",
    "day_range_one_day = [\"2016-04-26\", \"2016-06-03\"]\n",
    "df = df[df.date >= day_range[0]]\n",
    "df = df[df.date < day_range_one_day[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovani/anaconda3/envs/wavelet_code/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but NearestNeighbors was fitted without feature names\n",
      "  warnings.warn(\n",
      "100%|██████████| 5093564/5093564 [08:27<00:00, 10034.35it/s]\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(\"data/shapefiles/NYBlocks.geojson\")\n",
    "df = crossref(df, gdf)\n",
    "df = df.dropna(subset = [\"id_poly\"])\n",
    "df[\"id_poly\"] = df[\"id_poly\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_dates_3(day_range):\n",
    "    dates = pd.date_range(day_range[0], day_range[1], freq=\"D\")\n",
    "    # repeat with hours: 3h, 9h, 15h, 18h\n",
    "    new_dates = []\n",
    "    for d in dates:\n",
    "        for h in range(0, 24):\n",
    "            for m in [0]: #, 30]:\n",
    "                new_dates.append(d + pd.Timedelta(hours=h, minutes=m))\n",
    "        \n",
    "    return new_dates\n",
    "\n",
    "def simplify_date(x):\n",
    "    hour = x.hour\n",
    "    minute = 0 #x.minute // 30 * 30\n",
    "    return pd.to_datetime(f\"{x.date()} {hour}:{minute}:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dates = get_all_dates_3(day_range)\n",
    "len(all_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hour\"] = df.date.dt.hour\n",
    "df[\"minute\"] = 0 # df.date.dt.minute // 30 * 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>amount</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>date</th>\n",
       "      <th>id_poly</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1470742</th>\n",
       "      <td>2</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.86</td>\n",
       "      <td>9.3</td>\n",
       "      <td>-73.981743</td>\n",
       "      <td>40.759510</td>\n",
       "      <td>2016-04-26 00:00:00</td>\n",
       "      <td>572</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470743</th>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.00</td>\n",
       "      <td>9.8</td>\n",
       "      <td>-73.990685</td>\n",
       "      <td>40.743149</td>\n",
       "      <td>2016-04-26 00:00:00</td>\n",
       "      <td>305</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470744</th>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.3</td>\n",
       "      <td>-73.971489</td>\n",
       "      <td>40.794621</td>\n",
       "      <td>2016-04-26 00:00:00</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470746</th>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.19</td>\n",
       "      <td>7.3</td>\n",
       "      <td>-73.983688</td>\n",
       "      <td>40.755348</td>\n",
       "      <td>2016-04-26 00:00:00</td>\n",
       "      <td>968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470748</th>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.8</td>\n",
       "      <td>-73.997444</td>\n",
       "      <td>40.729797</td>\n",
       "      <td>2016-04-26 00:00:00</td>\n",
       "      <td>879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6564301</th>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.8</td>\n",
       "      <td>-73.959183</td>\n",
       "      <td>40.771816</td>\n",
       "      <td>2016-05-08 17:00:00</td>\n",
       "      <td>332</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6564302</th>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.8</td>\n",
       "      <td>-73.971657</td>\n",
       "      <td>40.787243</td>\n",
       "      <td>2016-05-08 17:00:00</td>\n",
       "      <td>1123</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6564303</th>\n",
       "      <td>2</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.3</td>\n",
       "      <td>-73.988327</td>\n",
       "      <td>40.748768</td>\n",
       "      <td>2016-05-08 17:00:00</td>\n",
       "      <td>1034</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6564304</th>\n",
       "      <td>2</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.76</td>\n",
       "      <td>8.8</td>\n",
       "      <td>-74.009827</td>\n",
       "      <td>40.705070</td>\n",
       "      <td>2016-05-08 17:00:00</td>\n",
       "      <td>1085</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6564305</th>\n",
       "      <td>1</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.8</td>\n",
       "      <td>-73.991287</td>\n",
       "      <td>40.729843</td>\n",
       "      <td>2016-05-08 17:00:00</td>\n",
       "      <td>583</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4223335 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         passenger_count  trip_distance  tip_amount  amount  LONGITUDE  \\\n",
       "1470742                2           1.74        1.86     9.3 -73.981743   \n",
       "1470743                1           1.80        2.00     9.8 -73.990685   \n",
       "1470744                2           1.50        0.00     9.3 -73.971489   \n",
       "1470746                3           0.90        2.19     7.3 -73.983688   \n",
       "1470748                1           0.70        1.00     6.8 -73.997444   \n",
       "...                  ...            ...         ...     ...        ...   \n",
       "6564301                2           0.50        1.00     4.8 -73.959183   \n",
       "6564302                1           0.67        0.00     4.8 -73.971657   \n",
       "6564303                2           0.84        0.00     6.3 -73.988327   \n",
       "6564304                2           1.28        1.76     8.8 -74.009827   \n",
       "6564305                1           0.60        0.00     5.8 -73.991287   \n",
       "\n",
       "          LATITUDE                date  id_poly  hour  minute  \n",
       "1470742  40.759510 2016-04-26 00:00:00      572     0       0  \n",
       "1470743  40.743149 2016-04-26 00:00:00      305     0       0  \n",
       "1470744  40.794621 2016-04-26 00:00:00      569     0       0  \n",
       "1470746  40.755348 2016-04-26 00:00:00      968     0       0  \n",
       "1470748  40.729797 2016-04-26 00:00:00      879     0       0  \n",
       "...            ...                 ...      ...   ...     ...  \n",
       "6564301  40.771816 2016-05-08 17:00:00      332    17       0  \n",
       "6564302  40.787243 2016-05-08 17:00:00     1123    17       0  \n",
       "6564303  40.748768 2016-05-08 17:00:00     1034    17       0  \n",
       "6564304  40.705070 2016-05-08 17:00:00     1085    17       0  \n",
       "6564305  40.729843 2016-05-08 17:00:00      583    17       0  \n",
       "\n",
       "[4223335 rows x 10 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = (\n",
    "    df[\"date\"].dt.floor(\"d\") + \n",
    "    pd.to_timedelta(df[\"hour\"], unit = \"h\") + \n",
    "    pd.to_timedelta(df[\"minute\"], unit = \"m\")\n",
    ")\n",
    "\n",
    "df = df.groupby([\"id_poly\", \"date\"]\n",
    "    ).agg(\n",
    "        {\"passenger_count\" : \"sum\", \"trip_distance\" : \"mean\", \"amount\" : \"mean\", \"tip_amount\" : \"mean\", \"hour\" : \"count\"}\n",
    "    ).reset_index()\n",
    "df.columns = [\"id_poly\", \"date\", \"Total Passengers\", \"Trip Distance\", \"Amount\", \"Tip Amount\", \"N Pickups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_poly = len(gdf)\n",
    "n_days = len(all_dates)\n",
    "\n",
    "columns = df.columns.tolist()[2:]\n",
    "\n",
    "ts = np.zeros((n_poly, n_days, len(columns)))\n",
    "\n",
    "for i, d in enumerate(all_dates):\n",
    "    filtered = df[df.date == d]\n",
    "    if len(filtered) == ts.shape[0]:\n",
    "        ts[:, i, :] = filtered[columns]\n",
    "    else:\n",
    "    # fill missing values with 0\n",
    "        ts[:, i] = 0\n",
    "        ts[filtered.id_poly, i, :] = filtered[columns]\n",
    "\n",
    "# change to a projection with meters as unit\n",
    "area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "ts = ts / area[:, None, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(columns)):\n",
    "    np.save(f\"data/time_series/{columns[i]}_NYBlocks_Period1.npy\", ts[:, :, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224256, 7)\n"
     ]
    }
   ],
   "source": [
    "poly_division = \"NYBlocks\"\n",
    "time_interval = \"Period1\"\n",
    "ts = [\n",
    "    np.load(f\"data/time_series/{signal}_{poly_division}_{time_interval}.npy\") for signal in columns\n",
    "]\n",
    "# drop if all values are 0\n",
    "ts_sum = [np.sum(t) for t in ts]\n",
    "ts = [t for i, t in enumerate(ts) if ts_sum[i] > 0]\n",
    "columns = [s for i, s in enumerate(columns) if ts_sum[i] > 0]\n",
    "df = []\n",
    "date = get_all_dates_3(day_range)\n",
    "for poly in range(ts[0].shape[0]):\n",
    "    for t in range(ts[0].shape[1]):\n",
    "        df.append({\n",
    "            \"date\" : date[t],\n",
    "            \"id_poly\" : poly,\n",
    "        })\n",
    "        for i, signal in enumerate(columns):\n",
    "            df[-1][signal] = ts[i][poly, t]\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "print(df.shape)\n",
    "df.to_csv(f\"data/polygon_data/{poly_division}_{time_interval}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little change to remove blocks without any signal\n",
    "df = pd.read_csv(\"data/polygon_data/NYBlocks_Period1.csv\")\n",
    "idx_to_drop = np.where(df.drop(columns = \"date\").groupby(\"id_poly\").max().max(axis = 1) == 0)[0]\n",
    "idx_to_keep = np.where(df.drop(columns = \"date\").groupby(\"id_poly\").max().max(axis = 1) != 0)[0]\n",
    "indexer = pd.DataFrame({\"id_poly\" : np.arange(len(df.id_poly.unique()))})\n",
    "indexer = indexer[~indexer.id_poly.isin(idx_to_drop)]\n",
    "indexer = indexer.reset_index(drop = True).to_dict()[\"id_poly\"]\n",
    "inv_indexer = {v : k for k, v in indexer.items()}\n",
    "\n",
    "\n",
    "\n",
    "gdf = gpd.read_file(\"data/shapefiles/NYBlocks.geojson\")\n",
    "gdf = gdf.drop(idx_to_drop)\n",
    "gdf[\"id_poly\"] = gdf[\"id_poly\"].map(inv_indexer)\n",
    "gdf.to_file(\"data/shapefiles/NYBlocks2.geojson\", driver = \"GeoJSON\")\n",
    "\n",
    "\n",
    "adj_matrix = np.load(\"data/adj_matrix/NYBlocks.npy\")\n",
    "adj_matrix = adj_matrix[idx_to_keep][:, idx_to_keep]\n",
    "np.save(\"data/adj_matrix/NYBlocks2.npy\", adj_matrix)\n",
    "\n",
    "df = pd.read_csv(\"data/polygon_data/NYBlocks_Period1.csv\")\n",
    "df = df[df.id_poly.isin(idx_to_keep)]\n",
    "df[\"id_poly\"] = df[\"id_poly\"].map(inv_indexer)\n",
    "df.to_csv(\"data/polygon_data/NYBlocks2_Period1.csv\", index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLA Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\"data/shapefiles/BLACities.geojson\")\n",
    "name = gdf.name.tolist()\n",
    "id_poly = gdf.id_poly.tolist()\n",
    "df = pd.read_csv(\"data/time_series/cities_time_series.csv\")\n",
    "df = df.drop(columns = ['nm_municip', 'x_start', 'y_start', 'x_width', 'y_width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "father_classes_hierarchy = {\n",
    "    \"floresta\": [\n",
    "        \"formacao_florestal\",\n",
    "        \"formacao_savanica\",\n",
    "        \"mangue\",\n",
    "        \"restinga_arborizada\",\n",
    "    ],\n",
    "    \"formacao_natural_nao_florestal\": [\n",
    "        \"campo_alagado_e_area_pantanosa\",\n",
    "        \"formacao_campestre\",\n",
    "        \"apicum\",\n",
    "        \"afloramento_rochoso\",\n",
    "        \"outras_formacoes_nao_florestais\",\n",
    "    ],\n",
    "    \"agropecuaria\": [\n",
    "        \"pastagem\",\n",
    "        \"soja\",\n",
    "        \"cana\",\n",
    "        \"arroz\",\n",
    "        \"outras_lavouras_temporarias\",\n",
    "        \"cafe\",\n",
    "        \"citrus\",\n",
    "        \"outras_lavouras_perenes\",\n",
    "        \"floresta_plantada\",\n",
    "        \"moisaco_de_agricultura_e_pastagem\",\n",
    "    ],\n",
    "    \"area_nao_vegetada\": [\n",
    "        \"praia_e_duna\",\n",
    "        \"infraestrutura_urbana\",\n",
    "        \"mineracao\",\n",
    "        \"outras_areas_nao_vegetadas\",\n",
    "    ],\n",
    "    \"corpo_dagua\": [\"rio_lago_e_oceano\", \"aquicultura\"],\n",
    "    \"nao_observado\": [\n",
    "        \"nao_observado\",  # \"nao_classificado\"\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for father, children in father_classes_hierarchy.items():\n",
    "    df[father] = df[children].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\"formacao_florestal\", \"formacao_savanica\", \"pastagem\", \"soja\", \"cana\", \"mineracao\", \"infraestrutura_urbana\", \"moisaco_de_agricultura_e_pastagem\", \"mangue\"]\n",
    "features_naming = {\n",
    "    \"formacao_florestal\" : \"Forest Formation\",\n",
    "    \"formacao_savanica\" : \"Cerrado\",\n",
    "    \"pastagem\" : \"Pasture\",\n",
    "    \"soja\" : \"Soy\",\n",
    "    \"cana\" : \"Sugar Cane\",\n",
    "    \"mineracao\" : \"Mining\",\n",
    "    \"infraestrutura_urbana\" : \"Urban Infrastructure\",\n",
    "    \"moisaco_de_agricultura_e_pastagem\" : \"Mosaic Agriculture\",\n",
    "    \"mangue\" : \"Mangrove\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"year\", \"cd_geocmu\"] + selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_yearly_diff(df):\n",
    "    df_copy = df.sort_values(\"year\")\n",
    "    for col in selected_columns:\n",
    "        df_copy[col] = df[col].diff()\n",
    "    df_copy = df_copy.dropna()\n",
    "    return df_copy\n",
    "    \n",
    "df_diff = df.groupby(\"cd_geocmu\").apply(calculate_yearly_diff, include_groups = False).reset_index()\n",
    "df_diff = df_diff.drop(columns = [\"level_1\"])\n",
    "df_diff[\"year\"] = df_diff.year.apply(lambda x : int(x - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = []\n",
    "for year in df.year.unique():\n",
    "    df_ = df[df.year == year]\n",
    "    new_df = {\n",
    "        \"id_poly\" : id_poly,\n",
    "    }\n",
    "    for column in selected_columns:\n",
    "        new_df[features_naming[column]] = df_[column]\n",
    "    new_df = pd.DataFrame(new_df)\n",
    "    new_df[\"date\"] = pd.to_datetime(f\"{year}-01-01\")\n",
    "    df_result.append(new_df)\n",
    "df_result = pd.concat(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place date and id_poly as first columns\n",
    "cols = df_result.columns.tolist()\n",
    "cols.remove(\"date\")\n",
    "cols.remove(\"id_poly\")\n",
    "cols = [\"date\", \"id_poly\"] + cols\n",
    "df_result = df_result[cols]\n",
    "df_result = df_result.sort_values([\"id_poly\", \"date\"])\n",
    "df_result.to_csv(\"data/polygon_data/BLACities_Year.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = []\n",
    "for year in df_diff.year.unique():\n",
    "    df_ = df_diff[df_diff.year == year]\n",
    "    new_df = {\n",
    "        \"id_poly\" : id_poly,\n",
    "    }\n",
    "    for column in selected_columns:\n",
    "        new_df[features_naming[column]] = df_[column]\n",
    "    new_df = pd.DataFrame(new_df)\n",
    "    new_df[\"date\"] = pd.to_datetime(f\"{year}-01-01\")\n",
    "    df_result.append(new_df)\n",
    "df_result = pd.concat(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place date and id_poly as first columns\n",
    "cols = df_result.columns.tolist()\n",
    "cols.remove(\"date\")\n",
    "cols.remove(\"id_poly\")\n",
    "cols = [\"date\", \"id_poly\"] + cols\n",
    "df_result = df_result[cols]\n",
    "df_result = df_result.sort_values([\"id_poly\", \"date\"])\n",
    "df_result.to_csv(\"data/polygon_data/BLACities_Year2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavelet_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
