{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9054/184408677.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CityHub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Point\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NearestNeighbors\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mCityHub\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'CityHub'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from shapely.geometry import Point\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import CityHub\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossref(df, gdf):\n",
    "    \"\"\"\n",
    "    Link rows of a dataframe to polygons in a geodataframe.\n",
    "    Dataframe must have columns 'LONGITUDE' and 'LATITUDE'.\n",
    "    \"\"\"\n",
    "    shapes = gdf.geometry\n",
    "    centroids = [x.centroid for x in shapes]\n",
    "    centroids = np.array([(x.x, x.y) for x in centroids])\n",
    "    nn = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(centroids)\n",
    "    _, indices = nn.kneighbors(df[['LONGITUDE', 'LATITUDE']])\n",
    "\n",
    "    id_poly = []\n",
    "    i = 0\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        point = Point(row['LONGITUDE'], row['LATITUDE'])\n",
    "        new_id = None\n",
    "        for idx in indices[i]:\n",
    "            if shapes[idx].contains(point):\n",
    "                new_id = idx\n",
    "                break\n",
    "        id_poly.append(new_id)\n",
    "        i += 1\n",
    "\n",
    "    df['id_poly'] = id_poly\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## São Paulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_range = [\"2019-12-01\", \"2020-02-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_mapper = {\n",
    "    \"A NOITE\" : \"21:00:00\",\n",
    "    \"A TARDE\" : \"15:00:00\",\n",
    "    \"DE MADRUGADA\" : \"03:00:00\",\n",
    "    \"PELA MANHÃ\" : \"09:00:00\",\n",
    "    \"EM HORA INCERTA\" : None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_dates(day_range):\n",
    "    dates = pd.date_range(day_range[0], day_range[1], freq=\"D\")\n",
    "    # repeat with hours: 3h, 9h, 15h, 18h\n",
    "    new_dates = []\n",
    "    for d in dates:\n",
    "        new_dates.append(d + pd.Timedelta(hours=3))\n",
    "        new_dates.append(d + pd.Timedelta(hours=9))\n",
    "        new_dates.append(d + pd.Timedelta(hours=15))\n",
    "        new_dates.append(d + pd.Timedelta(hours=21))\n",
    "    return new_dates\n",
    "\n",
    "def get_all_dates_2(day_range):\n",
    "    dates = pd.date_range(day_range[0], day_range[1], freq=\"D\")\n",
    "    # repeat with hours: 3h, 9h, 15h, 18h\n",
    "    new_dates = []\n",
    "    for d in dates:\n",
    "        new_dates.append(d + pd.Timedelta(hours=0))\n",
    "        new_dates.append(d + pd.Timedelta(hours=12))\n",
    "    return new_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_date(x):\n",
    "    hour = x.hour\n",
    "    if 6 <= hour < 12:\n",
    "        new_hour = 9\n",
    "    elif 12 <= hour < 18:\n",
    "        new_hour = 15\n",
    "    elif 18 <= hour < 24:\n",
    "        new_hour = 21\n",
    "    else:\n",
    "        new_hour = 3\n",
    "    return pd.to_datetime(f\"{x.date()} {new_hour}:00:00\")\n",
    "\n",
    "def simplify_date_2(x):\n",
    "    hour = x.hour\n",
    "    if 6 <= hour < 18:\n",
    "        new_hour = 12\n",
    "    else:\n",
    "        new_hour = 0\n",
    "    return pd.to_datetime(f\"{x.date()} {new_hour}:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_division = \"SpCenterCensus2k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000000\n",
    "df = pd.read_csv(\"data/time_series/waze-alerts.csv\")\n",
    "df = df[[\"geo\", \"ts\", \" type\"]]\n",
    "df = df.rename(columns = {\" type\" : \"type\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"ts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.date >= day_range[0]]\n",
    "df = df[df.date < day_range[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"LONGITUDE\"] = df[\"geo\"].apply(lambda x : x.split(\"(\")[1].split(\" \")[0])\n",
    "df[\"LATITUDE\"] = df[\"geo\"].apply(lambda x : x.split(\" \")[1].split(\")\")[0])\n",
    "df.LONGITUDE = df.LONGITUDE.astype(float)\n",
    "df.LATITUDE = df.LATITUDE.astype(float)\n",
    "df[\"type\"] = df[\"type\"].apply(lambda x : \"ROADCLOSED\" if x == \"ROAD_CLOSED\" else x)\n",
    "df = df[df.date >= day_range[0]]\n",
    "df = df[df.date < day_range[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovani/anaconda3/envs/wavelet_code/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but NearestNeighbors was fitted without feature names\n",
      "  warnings.warn(\n",
      "100%|██████████| 1686772/1686772 [02:48<00:00, 9995.70it/s] \n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "df = crossref(df, gdf)\n",
    "df = df.dropna(subset = [\"id_poly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"updated_date\"] = df[\"date\"].apply(simplify_date)\n",
    "df[\"id_poly\"] = df[\"id_poly\"].astype(int)\n",
    "n_poly = len(gdf)\n",
    "all_dates = get_all_dates(day_range)\n",
    "n_days = len(all_dates)\n",
    "\n",
    "for t in df.type.unique():\n",
    "    df_ = df[df.type == t].copy()\n",
    "    df_ = df_.groupby([\"id_poly\", \"updated_date\"]).size().reset_index()\n",
    "    ts = np.zeros((n_poly, n_days))\n",
    "    for i, d in enumerate(all_dates):\n",
    "        filtered = df_[df_.updated_date == d]\n",
    "        if len(filtered) == ts.shape[0]:\n",
    "            ts[:, i] = filtered[0]\n",
    "        else:\n",
    "            ts[filtered.id_poly, i] = filtered[0]\n",
    "    \n",
    "    area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "    ts = ts / area[:, None]\n",
    "    np.save(f\"data/time_series/{t}_{poly_division}_Period1.npy\", ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"updated_date\"] = df[\"date\"].apply(simplify_date_2)\n",
    "df[\"id_poly\"] = df[\"id_poly\"].astype(int)\n",
    "n_poly = len(gdf)\n",
    "all_dates = get_all_dates_2(day_range)\n",
    "n_days = len(all_dates)\n",
    "\n",
    "for t in df.type.unique():\n",
    "    df_ = df[df.type == t].copy()\n",
    "    df_ = df_.groupby([\"id_poly\", \"updated_date\"]).size().reset_index()\n",
    "    ts = np.zeros((n_poly, n_days))\n",
    "    for i, d in enumerate(all_dates):\n",
    "        filtered = df_[df_.updated_date == d]\n",
    "        if len(filtered) == ts.shape[0]:\n",
    "            ts[:, i] = filtered[0]\n",
    "        else:\n",
    "            ts[filtered.id_poly, i] = filtered[0]\n",
    "    \n",
    "    area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "    ts = ts / area[:, None]\n",
    "    np.save(f\"data/time_series/{t}_{poly_division}_Period2.npy\", ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime theft and robbery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_theft():\n",
    "    df = pd.read_csv(\"data/time_series/furto_celular_2018_2022.csv\", sep = \";\")\n",
    "    df[\"hour\"] = df.PERIDOOCORRENCIA.apply(lambda x : hour_mapper[x])\n",
    "    df[\"date\"] = pd.to_datetime(df.DATAOCORRENCIA, format='%d/%m/%Y', errors='coerce')\n",
    "    df[\"date\"] = df[\"date\"] + pd.to_timedelta(df[\"hour\"], errors='coerce')\n",
    "    print(f\"Fraction of nan dates: {df.date.isna().mean():.2f}\")\n",
    "    df = df.dropna(subset = [\"date\"])\n",
    "    df = df[[\"date\", \"LATITUDE\", \"LONGITUDE\"]]\n",
    "    gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "    df = crossref(df, gdf)\n",
    "    df = df.dropna(subset = [\"id_poly\"])\n",
    "    df[\"id_poly\"] = df[\"id_poly\"].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_707069/2194761068.py:2: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/time_series/furto_celular_2018_2022.csv\", sep = \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of nan dates: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovani/anaconda3/envs/wavelet_code/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but NearestNeighbors was fitted without feature names\n",
      "  warnings.warn(\n",
      "100%|██████████| 239651/239651 [00:25<00:00, 9239.66it/s]\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "df = load_theft()\n",
    "df = df[df.date >= day_range[0]]\n",
    "df = df[df.date < day_range[1]]\n",
    "n_poly = len(gdf)\n",
    "all_dates = get_all_dates(day_range)\n",
    "n_days = len(all_dates)\n",
    "df = df.groupby([\"id_poly\", \"date\"]).size().reset_index()\n",
    "\n",
    "ts = np.zeros((n_poly, n_days))\n",
    "for i, d in enumerate(all_dates):\n",
    "    filtered = df[df.date == d]\n",
    "    if len(filtered) == ts.shape[0]:\n",
    "        ts[:, i] = filtered[0]\n",
    "    else:\n",
    "        ts[filtered.id_poly, i] = filtered[0]\n",
    "\n",
    "area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "ts = ts / area[:, None]\n",
    "np.save(f\"data/time_series/FurtoCelular_{poly_division}_Period1.npy\", ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_707069/2194761068.py:2: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/time_series/furto_celular_2018_2022.csv\", sep = \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of nan dates: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovani/anaconda3/envs/wavelet_code/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but NearestNeighbors was fitted without feature names\n",
      "  warnings.warn(\n",
      "100%|██████████| 239651/239651 [00:26<00:00, 9151.02it/s]\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "df = load_theft()\n",
    "df = df[df.date >= day_range[0]]\n",
    "df = df[df.date < day_range[1]]\n",
    "df[\"date\"] = df[\"date\"].apply(simplify_date_2)\n",
    "\n",
    "n_poly = len(gdf)\n",
    "all_dates = get_all_dates_2(day_range)\n",
    "n_days = len(all_dates)\n",
    "df = df.groupby([\"id_poly\", \"date\"]).size().reset_index()\n",
    "\n",
    "ts = np.zeros((n_poly, n_days))\n",
    "for i, d in enumerate(all_dates):\n",
    "    filtered = df[df.date == d]\n",
    "    if len(filtered) == ts.shape[0]:\n",
    "        ts[:, i] = filtered[0]\n",
    "    else:\n",
    "        ts[filtered.id_poly, i] = filtered[0]\n",
    "\n",
    "area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "ts = ts / area[:, None]\n",
    "np.save(f\"data/time_series/FurtoCelular_{poly_division}_Period2.npy\", ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime robbery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_robbery():\n",
    "    df = pd.read_csv(\"data/time_series/roubo_celular_2018_2022.csv\", sep = \";\")\n",
    "    df[\"hour\"] = df.PERIDOOCORRENCIA.apply(lambda x : hour_mapper[x])\n",
    "    df[\"date\"] = pd.to_datetime(df.DATAOCORRENCIA, format='%d/%m/%Y', errors='coerce')\n",
    "    df[\"date\"] = df[\"date\"] + pd.to_timedelta(df[\"hour\"], errors='coerce')\n",
    "    print(f\"Fraction of nan dates: {df.date.isna().mean():.2f}\")\n",
    "    df = df.dropna(subset = [\"date\"])\n",
    "    df = df[[\"date\", \"LATITUDE\", \"LONGITUDE\"]]\n",
    "    gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "    df = crossref(df, gdf)\n",
    "    df = df.dropna(subset = [\"id_poly\"])\n",
    "    df[\"id_poly\"] = df[\"id_poly\"].astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_707069/1937086392.py:2: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/time_series/roubo_celular_2018_2022.csv\", sep = \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of nan dates: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovani/anaconda3/envs/wavelet_code/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but NearestNeighbors was fitted without feature names\n",
      "  warnings.warn(\n",
      "100%|██████████| 623682/623682 [01:04<00:00, 9618.97it/s]\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "df = load_robbery()\n",
    "df = df[df.date >= day_range[0]]\n",
    "df = df[df.date < day_range[1]]\n",
    "n_poly = len(gdf)\n",
    "n_poly = len(gdf)\n",
    "all_dates = get_all_dates(day_range)\n",
    "n_days = len(all_dates)\n",
    "df = df.groupby([\"id_poly\", \"date\"]).size().reset_index()\n",
    "ts = np.zeros((n_poly, n_days))\n",
    "for i, d in enumerate(all_dates):\n",
    "    filtered = df[df.date == d]\n",
    "    if len(filtered) == ts.shape[0]:\n",
    "        ts[:, i] = filtered[0]\n",
    "    else:\n",
    "        ts[filtered.id_poly, i] = filtered[0]\n",
    "\n",
    "area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "ts = ts / area[:, None]\n",
    "np.save(f\"data/time_series/RouboCelular_{poly_division}_Period1.npy\", ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_707069/1937086392.py:2: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/time_series/roubo_celular_2018_2022.csv\", sep = \";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of nan dates: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovani/anaconda3/envs/wavelet_code/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but NearestNeighbors was fitted without feature names\n",
      "  warnings.warn(\n",
      "100%|██████████| 623682/623682 [01:05<00:00, 9542.87it/s]\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(f\"data/shapefiles/{poly_division}.geojson\")\n",
    "df = load_robbery()\n",
    "df = df[df.date >= day_range[0]]\n",
    "df = df[df.date < day_range[1]]\n",
    "df[\"date\"] = df[\"date\"].apply(simplify_date_2)\n",
    "n_poly = len(gdf)\n",
    "all_dates = get_all_dates_2(day_range)\n",
    "n_days = len(all_dates)\n",
    "df = df.groupby([\"id_poly\", \"date\"]).size().reset_index()\n",
    "ts = np.zeros((n_poly, n_days))\n",
    "for i, d in enumerate(all_dates):\n",
    "    filtered = df[df.date == d]\n",
    "    if len(filtered) == ts.shape[0]:\n",
    "        ts[:, i] = filtered[0]\n",
    "    else:\n",
    "        ts[filtered.id_poly, i] = filtered[0]\n",
    "\n",
    "area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "ts = ts / area[:, None]\n",
    "np.save(f\"data/time_series/RouboCelular_{poly_division}_Period2.npy\", ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_naming = {\n",
    "    \"JAM\": \"Jam\",\n",
    "    \"ACCIDENT\": \"Accident\",\n",
    "    \"ROADCLOSED\": \"Road Closed\",\n",
    "    \"WEATHERHAZARD\": \"Weather Hazard\",\n",
    "    \"FurtoCelular\": \"Phone Theft\",\n",
    "    \"RouboCelular\": \"Phone Robbery\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(504000, 8)\n"
     ]
    }
   ],
   "source": [
    "ts = [np.load(f\"data/time_series/{key}_{poly_division}_Period1.npy\") for key in features_naming.keys()]\n",
    "ts_sum = [np.sum(t) for t in ts]\n",
    "ts = [t for i, t in enumerate(ts) if ts_sum[i] > 0]\n",
    "signals = [s for i, s in enumerate(features_naming.keys()) if ts_sum[i] > 0]\n",
    "date = get_all_dates(day_range)\n",
    "df = []\n",
    "for poly in range(ts[0].shape[0]):\n",
    "    for t in range(ts[0].shape[1]):\n",
    "        df.append({\n",
    "            \"date\" : date[t],\n",
    "            \"id_poly\" : poly,\n",
    "        })\n",
    "        for i, signal in enumerate(signals):\n",
    "            df[-1][\n",
    "                features_naming[signal]\n",
    "            ] = ts[i][poly, t]\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "print(df.shape)\n",
    "df.to_csv(f\"data/polygon_data/{poly_division}_Period1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(252000, 8)\n"
     ]
    }
   ],
   "source": [
    "ts = [np.load(f\"data/time_series/{key}_{poly_division}_Period2.npy\") for key in features_naming.keys()]\n",
    "ts_sum = [np.sum(t) for t in ts]\n",
    "ts = [t for i, t in enumerate(ts) if ts_sum[i] > 0]\n",
    "signals = [s for i, s in enumerate(features_naming.keys()) if ts_sum[i] > 0]\n",
    "date = get_all_dates_2(day_range)\n",
    "df = []\n",
    "for poly in range(ts[0].shape[0]):\n",
    "    for t in range(ts[0].shape[1]):\n",
    "        df.append({\n",
    "            \"date\" : date[t],\n",
    "            \"id_poly\" : poly,\n",
    "        })\n",
    "        for i, signal in enumerate(signals):\n",
    "            df[-1][\n",
    "                features_naming[signal]\n",
    "            ] = ts[i][poly, t]\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "print(df.shape)\n",
    "df.to_csv(f\"data/polygon_data/{poly_division}_Period2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = \"april\"\n",
    "df = pd.read_csv(f\"data/time_series/2016_{month}_Yellow_Taxi_Trip_Data.csv\")\n",
    "df = df[df.trip_distance < 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21394/2960374767.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"date\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"])\n"
     ]
    }
   ],
   "source": [
    "df[\"LONGITUDE\"] = df[\"pickup_longitude\"]\n",
    "df[\"LATITUDE\"] = df[\"pickup_latitude\"]\n",
    "df[\"date\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"])\n",
    "df = df.drop(columns = [\n",
    "    'VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
    "    'pickup_longitude', 'pickup_latitude', 'RatecodeID', 'store_and_fwd_flag',\n",
    "    'dropoff_longitude', 'dropoff_latitude', 'payment_type',  'extra', \n",
    "    'mta_tax', 'tolls_amount', 'improvement_surcharge', 'fare_amount', 'PULocationID', 'DOLocationID'], \n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovani/anaconda3/envs/wavelet_code/lib/python3.10/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but NearestNeighbors was fitted without feature names\n",
      "  warnings.warn(\n",
      "100%|██████████| 11934328/11934328 [19:25<00:00, 10237.22it/s]\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(\"data/shapefiles/NYBlocks.geojson\")\n",
    "df = crossref(df, gdf)\n",
    "df = df.dropna(subset = [\"id_poly\"])\n",
    "df[\"id_poly\"] = df[\"id_poly\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_dates(day_range):\n",
    "    dates = pd.date_range(day_range[0], day_range[1], freq=\"D\")\n",
    "    # repeat with hours: 3h, 9h, 15h, 18h\n",
    "    new_dates = []\n",
    "    for d in dates:\n",
    "        for h in range(2, 24, 4):\n",
    "            new_dates.append(d + pd.Timedelta(hours=h))\n",
    "        \n",
    "    return new_dates\n",
    "\n",
    "def simplify_date(x):\n",
    "    hour = x.hour\n",
    "    new_hour = hour // 4 * 4 + 2\n",
    "    return pd.to_datetime(f\"{x.date()} {new_hour}:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hour\"] = df.date.dt.hour\n",
    "df[\"hour\"] = df.hour // 4 * 4 + 2\n",
    "df[\"date\"] = df[\"date\"].dt.floor(\"d\") + pd.to_timedelta(df[\"hour\"], unit = \"h\")\n",
    "df = df.groupby([\"id_poly\", \"date\"]\n",
    "    ).agg(\n",
    "        {\"passenger_count\" : [\"mean\", \"sum\"], \"trip_distance\" : \"mean\", \"tip_amount\" : \"mean\", \"total_amount\" : \"mean\", \"date\" : \"count\"}\n",
    "    ).reset_index()\n",
    "df.columns = [\"id_poly\", \"date\", \"Passenger Count\", \"Total Passengers\", \"Trip Distance\", \"Tip Amount\", \"Total Amount\", \"N Pickups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_range = [\"2016-04-01\", \"2016-05-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_poly = len(gdf)\n",
    "all_dates = get_all_dates(day_range)\n",
    "n_days = len(all_dates)\n",
    "\n",
    "columns = [\"Passenger Count\", \"Total Passengers\", \"Trip Distance\", \"Tip Amount\", \"Total Amount\", \"N Pickups\"]\n",
    "\n",
    "ts = np.zeros((n_poly, n_days, 6))\n",
    "\n",
    "for i, d in enumerate(all_dates):\n",
    "    filtered = df[df.date == d]\n",
    "    if len(filtered) == ts.shape[0]:\n",
    "        ts[:, i, :] = filtered[columns]\n",
    "    else:\n",
    "    # fill missing values with 0\n",
    "        ts[:, i] = 0\n",
    "        ts[filtered.id_poly, i, :] = filtered[columns]\n",
    "\n",
    "# change to a projection with meters as unit\n",
    "area = gdf.to_crs(\"EPSG:6933\").area.values / 10**6\n",
    "ts = ts / area[:, None, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(columns)):\n",
    "    np.save(f\"data/time_series/{columns[i]}_NYBlocks_Period1.npy\", ts[:, :, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217248, 8)\n"
     ]
    }
   ],
   "source": [
    "poly_division = \"NYBlocks\"\n",
    "time_interval = \"Period1\"\n",
    "ts = [\n",
    "    np.load(f\"data/time_series/{signal}_{poly_division}_{time_interval}.npy\") for signal in columns\n",
    "]\n",
    "# drop if all values are 0\n",
    "ts_sum = [np.sum(t) for t in ts]\n",
    "ts = [t for i, t in enumerate(ts) if ts_sum[i] > 0]\n",
    "columns = [s for i, s in enumerate(columns) if ts_sum[i] > 0]\n",
    "df = []\n",
    "date = get_all_dates(day_range)\n",
    "for poly in range(ts[0].shape[0]):\n",
    "    for t in range(ts[0].shape[1]):\n",
    "        df.append({\n",
    "            \"date\" : date[t],\n",
    "            \"id_poly\" : poly,\n",
    "        })\n",
    "        for i, signal in enumerate(columns):\n",
    "            df[-1][signal] = ts[i][poly, t]\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "print(df.shape)\n",
    "df.to_csv(f\"data/polygon_data/{poly_division}_{time_interval}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLA Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\"data/shapefiles/BLACities.geojson\")\n",
    "name = gdf.name.tolist()\n",
    "id_poly = gdf.id_poly.tolist()\n",
    "df = pd.read_csv(\"data/time_series/cities_time_series.csv\")\n",
    "df = df.drop(columns = ['nm_municip', 'x_start', 'y_start', 'x_width', 'y_width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "father_classes_hierarchy = {\n",
    "    \"floresta\": [\n",
    "        \"formacao_florestal\",\n",
    "        \"formacao_savanica\",\n",
    "        \"mangue\",\n",
    "        \"restinga_arborizada\",\n",
    "    ],\n",
    "    \"formacao_natural_nao_florestal\": [\n",
    "        \"campo_alagado_e_area_pantanosa\",\n",
    "        \"formacao_campestre\",\n",
    "        \"apicum\",\n",
    "        \"afloramento_rochoso\",\n",
    "        \"outras_formacoes_nao_florestais\",\n",
    "    ],\n",
    "    \"agropecuaria\": [\n",
    "        \"pastagem\",\n",
    "        \"soja\",\n",
    "        \"cana\",\n",
    "        \"arroz\",\n",
    "        \"outras_lavouras_temporarias\",\n",
    "        \"cafe\",\n",
    "        \"citrus\",\n",
    "        \"outras_lavouras_perenes\",\n",
    "        \"floresta_plantada\",\n",
    "        \"moisaco_de_agricultura_e_pastagem\",\n",
    "    ],\n",
    "    \"area_nao_vegetada\": [\n",
    "        \"praia_e_duna\",\n",
    "        \"infraestrutura_urbana\",\n",
    "        \"mineracao\",\n",
    "        \"outras_areas_nao_vegetadas\",\n",
    "    ],\n",
    "    \"corpo_dagua\": [\"rio_lago_e_oceano\", \"aquicultura\"],\n",
    "    \"nao_observado\": [\n",
    "        \"nao_observado\",  # \"nao_classificado\"\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for father, children in father_classes_hierarchy.items():\n",
    "    df[father] = df[children].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\"formacao_florestal\", \"formacao_savanica\", \"pastagem\", \"soja\", \"cana\", \"mineracao\", \"infraestrutura_urbana\", \"moisaco_de_agricultura_e_pastagem\", \"mangue\"]\n",
    "features_naming = {\n",
    "    \"formacao_florestal\" : \"Forest Formation\",\n",
    "    \"formacao_savanica\" : \"Cerrado\",\n",
    "    \"pastagem\" : \"Pasture\",\n",
    "    \"soja\" : \"Soy\",\n",
    "    \"cana\" : \"Sugar Cane\",\n",
    "    \"mineracao\" : \"Mining\",\n",
    "    \"infraestrutura_urbana\" : \"Urban Infrastructure\",\n",
    "    \"moisaco_de_agricultura_e_pastagem\" : \"Mosaic Agriculture\",\n",
    "    \"mangue\" : \"Mangrove\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"year\", \"cd_geocmu\"] + selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_yearly_diff(df):\n",
    "    df_copy = df.sort_values(\"year\")\n",
    "    for col in selected_columns:\n",
    "        df_copy[col] = df[col].diff()\n",
    "    df_copy = df_copy.dropna()\n",
    "    return df_copy\n",
    "    \n",
    "df_diff = df.groupby(\"cd_geocmu\").apply(calculate_yearly_diff, include_groups = False).reset_index()\n",
    "df_diff = df_diff.drop(columns = [\"level_1\"])\n",
    "df_diff[\"year\"] = df_diff.year.apply(lambda x : int(x - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = []\n",
    "for year in df.year.unique():\n",
    "    df_ = df[df.year == year]\n",
    "    new_df = {\n",
    "        \"id_poly\" : id_poly,\n",
    "    }\n",
    "    for column in selected_columns:\n",
    "        new_df[features_naming[column]] = df_[column]\n",
    "    new_df = pd.DataFrame(new_df)\n",
    "    new_df[\"date\"] = pd.to_datetime(f\"{year}-01-01\")\n",
    "    df_result.append(new_df)\n",
    "df_result = pd.concat(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place date and id_poly as first columns\n",
    "cols = df_result.columns.tolist()\n",
    "cols.remove(\"date\")\n",
    "cols.remove(\"id_poly\")\n",
    "cols = [\"date\", \"id_poly\"] + cols\n",
    "df_result = df_result[cols]\n",
    "df_result = df_result.sort_values([\"id_poly\", \"date\"])\n",
    "df_result.to_csv(\"data/polygon_data/BLACities_Year.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = []\n",
    "for year in df_diff.year.unique():\n",
    "    df_ = df_diff[df_diff.year == year]\n",
    "    new_df = {\n",
    "        \"id_poly\" : id_poly,\n",
    "    }\n",
    "    for column in selected_columns:\n",
    "        new_df[features_naming[column]] = df_[column]\n",
    "    new_df = pd.DataFrame(new_df)\n",
    "    new_df[\"date\"] = pd.to_datetime(f\"{year}-01-01\")\n",
    "    df_result.append(new_df)\n",
    "df_result = pd.concat(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place date and id_poly as first columns\n",
    "cols = df_result.columns.tolist()\n",
    "cols.remove(\"date\")\n",
    "cols.remove(\"id_poly\")\n",
    "cols = [\"date\", \"id_poly\"] + cols\n",
    "df_result = df_result[cols]\n",
    "df_result = df_result.sort_values([\"id_poly\", \"date\"])\n",
    "df_result.to_csv(\"data/polygon_data/BLACities_Year2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavelet_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
