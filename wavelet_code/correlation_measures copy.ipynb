{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134958/1165076934.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import correlate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation(poly_division, time_interval, max_lags = 5):\n",
    "    df = pd.read_csv(f\"data/polygon_data/{poly_division}_{time_interval}.csv\")\n",
    "    signal_multi = np.stack(df.groupby(\"id_poly\").apply(lambda x: x.sort_values(\"date\").drop(columns=[\"id_poly\", \"date\"]).values).values)\n",
    "    \n",
    "    features = df.columns.tolist()[2:]\n",
    "    dist_matrix = np.zeros((len(features), len(features)))\n",
    "\n",
    "    for i, sample in tqdm(enumerate(signal_multi)):\n",
    "        dist_matrix_ = np.zeros((len(features), len(features)))\n",
    "        for m in range(len(features)):\n",
    "            sample[:, m] -= sample[:, m].mean()\n",
    "            sample[:, m] /= sample[:, m].std() + 1e-6\n",
    "            for n in range(len(features)):\n",
    "                if m == n:\n",
    "                    continue\n",
    "                # calculate cross-correlation\n",
    "\n",
    "                \n",
    "                sample[:, n] -= sample[:, n].mean()\n",
    "                sample[:, n] /= sample[:, n].std() + 1e-6\n",
    "                xcorr = correlate(sample[:, m], sample[:, n], mode='full')\n",
    "                xcorr = max(xcorr, key=abs)\n",
    "                dist_matrix_[m, n] = xcorr\n",
    "                # normalize cross-correlation\n",
    "                div = np.sqrt(np.sum(sample[:, m]**2) * np.sum(sample[:, n]**2))\n",
    "                div = div if div != 0 else 1\n",
    "                dist_matrix_[m, n] /= div\n",
    "\n",
    "        dist_matrix += dist_matrix_\n",
    "\n",
    "    dist_matrix /= len(signal_multi)\n",
    "\n",
    "    \n",
    "    result = [\n",
    "        \n",
    "    ]\n",
    "\n",
    "    for i, feat_i in enumerate(features):\n",
    "        for j, feat_j in enumerate(features):\n",
    "            if feat_i == feat_j:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            result.append({\n",
    "                \"row\": feat_i,\n",
    "                \"column\": feat_j,\n",
    "                \"value\": dist_matrix[i, j],\n",
    "                \"lag\": -777\n",
    "            })\n",
    "\n",
    "            \n",
    "\n",
    "    result = pd.DataFrame(result)\n",
    "    result.to_csv(f\"data/similarity_matrix/{poly_division}_{time_interval}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation(poly_division, time_interval, max_lags = 5):\n",
    "    df = pd.read_csv(f\"data/polygon_data/{poly_division}_{time_interval}.csv\")\n",
    "    signal_multi = np.stack(df.groupby(\"id_poly\").apply(lambda x: x.sort_values(\"date\").drop(columns=[\"id_poly\", \"date\"]).values).values)\n",
    "    \n",
    "    features = df.columns.tolist()[2:]\n",
    "    dist_matrix = np.zeros((len(features), len(features)))\n",
    "\n",
    "    for i, sample in tqdm(enumerate(signal_multi)):\n",
    "        dist_matrix_ = np.zeros((len(features), len(features)))\n",
    "        for m in range(len(features)):\n",
    "            if np.sum(sample[:, m]) == 0:\n",
    "                continue\n",
    "            for n in range(len(features)):\n",
    "                if m == n:\n",
    "                    continue\n",
    "                if np.sum(sample[:, n]) == 0:\n",
    "                    continue\n",
    "                # calculate correlation\n",
    "                \n",
    "                xcorr = pearsonr(sample[:, m], sample[:, n])[0]\n",
    "                if xcorr != 1:\n",
    "                    xcorr = np.arctanh(xcorr)\n",
    "                dist_matrix_[m, n] = xcorr\n",
    "\n",
    "        \n",
    "        dist_matrix += dist_matrix_\n",
    "\n",
    "        dist_matrix__ = dist_matrix / (i + 1)\n",
    "        dist_matrix__ = np.tanh(dist_matrix__)\n",
    "        if dist_matrix__.max() >= 1:\n",
    "            print(i, dist_matrix_, dist_matrix)\n",
    "            return\n",
    "\n",
    "    dist_matrix /= len(signal_multi)\n",
    "    dist_matrix[dist_matrix > 0] = np.tanh(dist_matrix[dist_matrix > 0])\n",
    "    \n",
    "    result = [\n",
    "        \n",
    "    ]\n",
    "\n",
    "    for i, feat_i in enumerate(features):\n",
    "        for j, feat_j in enumerate(features):\n",
    "            if feat_i == feat_j:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            result.append({\n",
    "                \"row\": feat_i,\n",
    "                \"column\": feat_j,\n",
    "                \"value\": dist_matrix[i, j],\n",
    "                \"lag\": -777\n",
    "            })\n",
    "\n",
    "            \n",
    "\n",
    "    result = pd.DataFrame(result)\n",
    "    result.to_csv(f\"data/similarity_matrix/{poly_division}_{time_interval}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation(poly_division, time_interval, max_lags = 5):\n",
    "    df = pd.read_csv(f\"data/polygon_data/{poly_division}_{time_interval}.csv\")\n",
    "    \n",
    "    # create lagged columns\n",
    "    df_new = df.copy()\n",
    "    for lag in range(-max_lags, max_lags + 1):\n",
    "        df_new = pd.merge(\n",
    "            df_new,\n",
    "            df.groupby(\"id_poly\").apply(lambda x : x.set_index(\"date\").shift(lag), include_groups=False).reset_index(),\n",
    "            on=[\"id_poly\", \"date\"],\n",
    "            how = \"inner\",\n",
    "            suffixes = (\"\", f\"_lag_{lag}\")\n",
    "        )\n",
    "\n",
    "    df_new.to_csv(f\"data/polygon_data/{poly_division}_{time_interval}_lagged.csv\", index=False)\n",
    "    corr_matrix = df_new.iloc[:, 2:].corr()\n",
    "    features = df.columns.tolist()[2:]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i, feat_i in enumerate(features):\n",
    "        correlations = corr_matrix[feat_i].to_dict()\n",
    "        for j, feat_j in enumerate(features):\n",
    "            if feat_i == feat_j:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            correlations_values = [correlations[feat_j + f\"_lag_{lag}\"] for lag in range(-max_lags, max_lags + 1)]\n",
    "\n",
    "            # get lag and value of maximum absolute correlation\n",
    "            max_corr = max(correlations_values, key=abs)\n",
    "            max_corr_lag = correlations_values.index(max_corr) - max_lags\n",
    "\n",
    "            result.append({\n",
    "                \"row\": feat_i,\n",
    "                \"column\": feat_j,\n",
    "                \"value\": max_corr,\n",
    "                \"lag\": max_corr_lag\n",
    "            })\n",
    "\n",
    "            \n",
    "\n",
    "    result = pd.DataFrame(result)\n",
    "    result.to_csv(f\"data/similarity_matrix/{poly_division}_{time_interval}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_correlation(\"SpCenterCensus5k\", \"Period2\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_correlation(\"BLACities\", \"Year2\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_correlation(\"NYBlocks\", \"Period1\", 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavelet_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
